{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento com BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import funcoes_bert as fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# force CPU (make CPU visible)\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "print(cpus)\n",
    "tf.config.set_visible_devices([], 'GPU')  # hide the GPU\n",
    "tf.config.set_visible_devices(cpus[0], 'CPU') # unhide potentially hidden CPU\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = ['hn_balanceado', 'hn_desbalanceado', 'sn_balanceado', 'sn_desbalanceado']\n",
    "\n",
    "lista_df = []\n",
    "for arquivo in arquivos:\n",
    "\n",
    "    df = pd.read_csv(f\"Dados/Datasets/{arquivo}.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    lista_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descricao_1    object\n",
       "ean_1          object\n",
       "titulo_1       object\n",
       "url_1          object\n",
       "titulo_cb_1    object\n",
       "loja_1         object\n",
       "descricao_2    object\n",
       "ean_2          object\n",
       "titulo_2       object\n",
       "url_2          object\n",
       "titulo_cb_2    object\n",
       "loja_2         object\n",
       "match           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_df[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13290, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descricao_1</th>\n",
       "      <th>ean_1</th>\n",
       "      <th>titulo_1</th>\n",
       "      <th>url_1</th>\n",
       "      <th>titulo_cb_1</th>\n",
       "      <th>loja_1</th>\n",
       "      <th>descricao_2</th>\n",
       "      <th>ean_2</th>\n",
       "      <th>titulo_2</th>\n",
       "      <th>url_2</th>\n",
       "      <th>titulo_cb_2</th>\n",
       "      <th>loja_2</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Esmaltec traz o fogão feito especialmente para...</td>\n",
       "      <td>7899081743783</td>\n",
       "      <td>Fogão 4 Bocas a Gás Esmaltec Esmeralda Glass 4...</td>\n",
       "      <td>https://www.amazon.com.br/Esmaltec-Esmeralda-A...</td>\n",
       "      <td>fogão 4 bocas a gás esmaltec esmeralda glass 4...</td>\n",
       "      <td>amazon</td>\n",
       "      <td>Esmaltec traz o fogão feito especialmente para...</td>\n",
       "      <td>7899081743783</td>\n",
       "      <td>Fogão 4 Bocas a Gás Esmaltec Esmeralda Glass 4...</td>\n",
       "      <td>https://www.americanas.com.br/produto/2599745729</td>\n",
       "      <td>fogão 4 bocas a gás esmaltec esmeralda glass 4...</td>\n",
       "      <td>americanas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7892509119160</td>\n",
       "      <td>Smartphone Samsung Galaxy S20 Fe 128GB 4G Wi-F...</td>\n",
       "      <td>https://www.amazon.com.br/Smartphone-Samsung-G...</td>\n",
       "      <td>smartphone samsung galaxy s20 fe 128gb 4g wi-f...</td>\n",
       "      <td>amazon</td>\n",
       "      <td>Todas as informações divulgadas são de respons...</td>\n",
       "      <td>7892509119160</td>\n",
       "      <td>Smartphone Samsung Galaxy S20 Fe 128GB 4G Wi-F...</td>\n",
       "      <td>https://www.americanas.com.br/produto/3234381133</td>\n",
       "      <td>smartphone samsung galaxy s20 fe 128gb 4g wi-f...</td>\n",
       "      <td>americanas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10407</th>\n",
       "      <td>Todas as informações divulgadas são de respons...</td>\n",
       "      <td>7892509118439</td>\n",
       "      <td>Smartphone Samsung Galaxy A32 128GB 4G Wi-Fi T...</td>\n",
       "      <td>https://www.americanas.com.br/produto/2969968331</td>\n",
       "      <td>smartphone samsung galaxy a32 128gb 4g wi-fi t...</td>\n",
       "      <td>americanas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7892509118439</td>\n",
       "      <td>Smartphone Samsung Galaxy A32 128GB 4G Wi-Fi T...</td>\n",
       "      <td>https://www.amazon.com.br/Smartphone-Samsung-G...</td>\n",
       "      <td>smartphone samsung galaxy a32 128gb 4g wi-fi t...</td>\n",
       "      <td>amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             descricao_1          ean_1  \\\n",
       "2231   Esmaltec traz o fogão feito especialmente para...  7899081743783   \n",
       "7817                                                 NaN  7892509119160   \n",
       "10407  Todas as informações divulgadas são de respons...  7892509118439   \n",
       "\n",
       "                                                titulo_1  \\\n",
       "2231   Fogão 4 Bocas a Gás Esmaltec Esmeralda Glass 4...   \n",
       "7817   Smartphone Samsung Galaxy S20 Fe 128GB 4G Wi-F...   \n",
       "10407  Smartphone Samsung Galaxy A32 128GB 4G Wi-Fi T...   \n",
       "\n",
       "                                                   url_1  \\\n",
       "2231   https://www.amazon.com.br/Esmaltec-Esmeralda-A...   \n",
       "7817   https://www.amazon.com.br/Smartphone-Samsung-G...   \n",
       "10407   https://www.americanas.com.br/produto/2969968331   \n",
       "\n",
       "                                             titulo_cb_1      loja_1  \\\n",
       "2231   fogão 4 bocas a gás esmaltec esmeralda glass 4...      amazon   \n",
       "7817   smartphone samsung galaxy s20 fe 128gb 4g wi-f...      amazon   \n",
       "10407  smartphone samsung galaxy a32 128gb 4g wi-fi t...  americanas   \n",
       "\n",
       "                                             descricao_2          ean_2  \\\n",
       "2231   Esmaltec traz o fogão feito especialmente para...  7899081743783   \n",
       "7817   Todas as informações divulgadas são de respons...  7892509119160   \n",
       "10407                                                NaN  7892509118439   \n",
       "\n",
       "                                                titulo_2  \\\n",
       "2231   Fogão 4 Bocas a Gás Esmaltec Esmeralda Glass 4...   \n",
       "7817   Smartphone Samsung Galaxy S20 Fe 128GB 4G Wi-F...   \n",
       "10407  Smartphone Samsung Galaxy A32 128GB 4G Wi-Fi T...   \n",
       "\n",
       "                                                   url_2  \\\n",
       "2231    https://www.americanas.com.br/produto/2599745729   \n",
       "7817    https://www.americanas.com.br/produto/3234381133   \n",
       "10407  https://www.amazon.com.br/Smartphone-Samsung-G...   \n",
       "\n",
       "                                             titulo_cb_2      loja_2  match  \n",
       "2231   fogão 4 bocas a gás esmaltec esmeralda glass 4...  americanas      1  \n",
       "7817   smartphone samsung galaxy s20 fe 128gb 4g wi-f...  americanas      1  \n",
       "10407  smartphone samsung galaxy a32 128gb 4g wi-fi t...      amazon      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"titulo_1\"]==df[\"titulo_2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizador e Inicialização do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test = None\\n\\n# can be up to 512 for BERT\\nMAX_LENGTH = 256\\nBATCH_SIZE = 32\\ntokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case = False)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test = None\n",
    "\n",
    "# can be up to 512 for BERT\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case = False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#######################INICIOS FUNÇÕES DE APOIO#######################\\n\\ndef map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\\n  return {\\n      \"input_ids\": input_ids,\\n      \"token_type_ids\": token_type_ids,\\n      \"attention_mask\": attention_masks,\\n  }, label\\n\\n\\ndef convert_example_to_feature(titulo_1, titulo_2):\\n    return tokenizer.encode_plus(titulo_1, titulo_2,\\n                                 add_special_tokens = True, # adiciona [CLS], [SEP]\\n                                 max_length = MAX_LENGTH, # comprimento máximo do texto de entrada\\n                                 padding = \\'max_length\\', # adiciona [PAD] até o tam_max (MAX_LENGTH)\\n                                 truncation = True, # padrão = \\'longest_first\\'\\n                                 return_attention_mask = True, # adiciona máscara de atenção para não focar nos tokens do pad\\n                                )\\n\\ndef encode_examples(df_titulos, labels, limit = -1):\\n    \\n    # prepare list, so that we can build up final TensorFlow dataset from slices.\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n    label_list = []\\n    \\n    if (limit > 0):\\n        ds = ds.take(limit)\\n    \\n    # for review, label in tfds.as_numpy(ds):\\n    for titulo_1, titulo_2, label in zip(df_titulos[\"titulo_1\"], df_titulos[\"titulo_2\"], labels):\\n        \\n        bert_input = convert_example_to_feature(titulo_1, titulo_2)\\n        input_ids_list.append(bert_input[\\'input_ids\\'])\\n        token_type_ids_list.append(bert_input[\\'token_type_ids\\'])\\n        attention_mask_list.append(bert_input[\\'attention_mask\\'])\\n        label_list.append([label])\\n        \\n    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\\n\\ndef get_bert_data(X_train, y_train, X_valid, y_valid, X_test, y_test):\\n    \\n    # train dataset\\n    ds_train = encode_examples(X_train, y_train).batch(BATCH_SIZE)\\n\\n    # test dataset\\n    ds_test = encode_examples(X_test, y_test).batch(BATCH_SIZE)\\n\\n    #validation dataset\\n    ds_valid = encode_examples(X_valid, y_valid).batch(BATCH_SIZE)\\n\\n    return ds_train, ds_valid, ds_test\\n\\n#######################FIM FUNÇOES DE APOIO#######################\\n\\ndef get_test_metrics(model, ds_test, y_test):\\n\\n    #Predictin test dataset\\n    tf_output = model.predict(ds_test)[0]\\n    tf_prediction = tf.nn.softmax(tf_output, axis=1)\\n    # labels = [\\'Negative\\',\\'Positive\\'] #(0:negative, 1:positive)\\n    label = tf.argmax(tf_prediction, axis=1)\\n    label_pred = label.numpy()\\n    # print(label_pred)\\n\\n    print(classification_report(y_test, label_pred))\\n\\n    print(confusion_matrix(y_test, label_pred))\\n\\n    return label_pred\\n\\n\\ndef pipeline_bert(name, X_train, y_train, X_valid, y_valid, X_test, y_test): #X_train = [titulos1, titulos2]\\n\\n    learning_rate = 2e-5\\n    number_of_epochs = 3\\n    ds_train, ds_valid, ds_test = get_bert_data(X_train, y_train, X_valid, y_valid, X_test, y_test)\\n    \\n    # model initialization\\n    model = TFBertForSequenceClassification.from_pretrained(\\'neuralmind/bert-base-portuguese-cased\\', from_pt = True)\\n\\n    # choosing Adam optimizer\\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n    metric_acc = tf.keras.metrics.SparseCategoricalAccuracy(\\'accuracy\\')\\n    model.compile(optimizer=optimizer, loss=loss, metrics=[metric_acc])\\n\\n    #Training model\\n    bert_history = model.fit(ds_train, epochs=number_of_epochs, validation_data=ds_valid)\\n    \\n    #Predict test data\\n    result = get_test_metrics(model, ds_test, y_test)\\n    return result\\n\\n    # metrics = calc_metrics(y_test, result, name)\\n    # return metrics\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#######################INICIOS FUNÇÕES DE APOIO#######################\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "\n",
    "def convert_example_to_feature(titulo_1, titulo_2):\n",
    "    return tokenizer.encode_plus(titulo_1, titulo_2,\n",
    "                                 add_special_tokens = True, # adiciona [CLS], [SEP]\n",
    "                                 max_length = MAX_LENGTH, # comprimento máximo do texto de entrada\n",
    "                                 padding = 'max_length', # adiciona [PAD] até o tam_max (MAX_LENGTH)\n",
    "                                 truncation = True, # padrão = 'longest_first'\n",
    "                                 return_attention_mask = True, # adiciona máscara de atenção para não focar nos tokens do pad\n",
    "                                )\n",
    "\n",
    "def encode_examples(df_titulos, labels, limit = -1):\n",
    "    \n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    \n",
    "    # for review, label in tfds.as_numpy(ds):\n",
    "    for titulo_1, titulo_2, label in zip(df_titulos[\"titulo_1\"], df_titulos[\"titulo_2\"], labels):\n",
    "        \n",
    "        bert_input = convert_example_to_feature(titulo_1, titulo_2)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "        \n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "def get_bert_data(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    \n",
    "    # train dataset\n",
    "    ds_train = encode_examples(X_train, y_train).batch(BATCH_SIZE)\n",
    "\n",
    "    # test dataset\n",
    "    ds_test = encode_examples(X_test, y_test).batch(BATCH_SIZE)\n",
    "\n",
    "    #validation dataset\n",
    "    ds_valid = encode_examples(X_valid, y_valid).batch(BATCH_SIZE)\n",
    "\n",
    "    return ds_train, ds_valid, ds_test\n",
    "\n",
    "#######################FIM FUNÇOES DE APOIO#######################\n",
    "\n",
    "def get_test_metrics(model, ds_test, y_test):\n",
    "\n",
    "    #Predictin test dataset\n",
    "    tf_output = model.predict(ds_test)[0]\n",
    "    tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "    # labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "    label = tf.argmax(tf_prediction, axis=1)\n",
    "    label_pred = label.numpy()\n",
    "    # print(label_pred)\n",
    "\n",
    "    print(classification_report(y_test, label_pred))\n",
    "\n",
    "    print(confusion_matrix(y_test, label_pred))\n",
    "\n",
    "    return label_pred\n",
    "\n",
    "\n",
    "def pipeline_bert(name, X_train, y_train, X_valid, y_valid, X_test, y_test): #X_train = [titulos1, titulos2]\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    number_of_epochs = 3\n",
    "    ds_train, ds_valid, ds_test = get_bert_data(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "    \n",
    "    # model initialization\n",
    "    model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', from_pt = True)\n",
    "\n",
    "    # choosing Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric_acc = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric_acc])\n",
    "\n",
    "    #Training model\n",
    "    bert_history = model.fit(ds_train, epochs=number_of_epochs, validation_data=ds_valid)\n",
    "    \n",
    "    #Predict test data\n",
    "    result = get_test_metrics(model, ds_test, y_test)\n",
    "    return result\n",
    "\n",
    "    # metrics = calc_metrics(y_test, result, name)\n",
    "    # return metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando pipeline experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 42s 42s/step - loss: 0.5555 - accuracy: 0.8000 - val_loss: 0.5317 - val_accuracy: 0.8500\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 22s 22s/step - loss: 0.5244 - accuracy: 0.8500 - val_loss: 0.4838 - val_accuracy: 0.8500\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 22s 22s/step - loss: 0.4699 - accuracy: 0.8500 - val_loss: 0.4435 - val_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "lista_df_resultado = []\n",
    "for nome, df in zip(arquivos, lista_df):\n",
    "    \n",
    "    X = df[[\"titulo_1\", \"titulo_2\"]]#[df[\"titulo_1\"].to_list(), df[\"titulo_2\"].to_list()]\n",
    "    y = df[\"match\"].to_list()\n",
    "\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED, stratify = y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2, random_state = SEED, stratify = y_train_valid)\n",
    "\n",
    "    (nome, historico, y_test, y_pred) = fb.pipeline_bert(nome, X_train[:20], y_train[:20], X_valid[:20], y_valid[:20], X_test[:20], y_test[:20])\n",
    "\n",
    "    pd.DataFrame.from_dict(historico.history).to_csv(f'Dados/Resultados/{nome}_historico.csv', index = False)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "    df_resultado = pd.DataFrame(report).transpose()\n",
    "    df_resultado['modelo'] = nome\n",
    "\n",
    "    lista_df_resultado.append(df_resultado)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        18\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.45      0.50      0.47        20\n",
      "weighted avg       0.81      0.90      0.85        20\n",
      "\n",
      "[[18  0]\n",
      " [ 2  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4889b298c2b41a0f37c917b2884deb5a3a36b040cfb99d0e8c1edb9e5e6fadf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
