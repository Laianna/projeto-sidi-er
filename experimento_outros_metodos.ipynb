{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import funcoes_modelos as fmod\n",
    "import funcoes_bow as fbow\n",
    "import funcoes_cos as fcos\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag_cpu = True\n",
    "# if flag_cpu == True:\n",
    "#     # force CPU (make CPU visible)\n",
    "#     cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "#     print(cpus)\n",
    "#     tf.config.set_visible_devices([], 'GPU')  # hide the GPU\n",
    "#     tf.config.set_visible_devices(cpus[0], 'CPU') # unhide potentially hidden CPU\n",
    "#     tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = ['hn_balanceado', 'hn_desbalanceado', 'sn_balanceado', 'sn_desbalanceado']\n",
    "\n",
    "lista_df = []\n",
    "for arquivo in arquivos:\n",
    "\n",
    "    df = pd.read_csv(f\"Dados/Datasets/{arquivo}.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    lista_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_df[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Tamanho dos Datasets:\\n\\n\\t\\t| Hard\\t| Soft\\t|\\nBalanceado\\t| {lista_df[0].shape[0]}\\t| {lista_df[2].shape[0]}\\t|\\nDesbalanceado\\t| {lista_df[1].shape[0]}\\t| {lista_df[3].shape[0]}\\t|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_valid(df):\n",
    "    \n",
    "    X = df[[\"titulo_1\", \"titulo_2\"]]\n",
    "    y = df[\"match\"].to_list()\n",
    "    \n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED, stratify = y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.285, random_state = SEED, stratify = y_train_valid)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_train_test_valid = []\n",
    "for df in lista_df:\n",
    "    lista_train_test_valid.append(train_test_valid(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nome, dataset in zip(arquivos, lista_train_test_valid):    \n",
    "#    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos Com Os Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df_resultado(report, modelo, nome_dataset, tempo_exec):\n",
    "    \n",
    "    df_resultado = pd.DataFrame(report).transpose()\n",
    "    df_resultado['modelo'] = modelo\n",
    "    df_resultado['dataset'] = nome_dataset\n",
    "    df_resultado['tempo'] = tempo_exec\n",
    "\n",
    "    df_resultado.to_csv(f'Dados/Resultados/{modelo}/{nome_dataset}_resultado.csv', index = True)\n",
    "    \n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "def salvar_modelo(modelo, dataset, metodo):\n",
    "    \n",
    "    nome_arquivo = f\"Dados/Modelos/modelo_{dataset}_{metodo}\"\n",
    "    pickle.dump(modelo, open(nome_arquivo, 'wb'))\n",
    "\n",
    "def carregar_modelo(nome, metodo):\n",
    "    \n",
    "    modelo = pickle.load(open(f\"Dados/Modelos/modelo_{nome}_{metodo}\", 'rb'))\n",
    "    return modelo\n",
    "\n",
    "\n",
    "def salvar_modelo_bert(modelo, dataset, metodo):\n",
    "    \n",
    "    nome_arquivo = f\"Dados/Modelos/{metodo}/{dataset}/\"\n",
    "    modelo.save_pretrained(nome_arquivo)\n",
    "\n",
    "def carregar_modelo_bert(nome, dataset, metodo):\n",
    "    \n",
    "    from transformers import TFAutoModel\n",
    "    \n",
    "    modelo = TFAutoModel.from_pretrained(f\"Dados/Modelos/{metodo}/{dataset}/\")\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT, roBERTa, XLMR e ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodos = (\"BERT\", \"roBERTa\",  \"XLMR\", \"ELECTRA\")\n",
    "metodos = (\"roBERTa\", \"XLMR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 119, in _create_slots\n        self.add_slot(var, 'v')\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 911, in add_slot\n        weight = tf.Variable(\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 145, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[150000,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21664/3987522390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#(nome, historico, y_test, y_pred, modelo) = fmod.pipeline_bert(metodo, nome, X_train, y_train, X_valid, y_valid, X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mnome\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistorico\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetodo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnome\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\llvs2\\Desktop\\UFPE\\SiDi\\projeto-sidi-er\\funcoes_modelos.py\u001b[0m in \u001b[0;36mpipeline_bert\u001b[1;34m(name_model, name_dataset, X_train, y_train, X_valid, y_valid, X_test, y_test)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;31m#Training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mbert_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumber_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m#Predict test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 119, in _create_slots\n        self.add_slot(var, 'v')\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 911, in add_slot\n        weight = tf.Variable(\n    File \"c:\\Users\\llvs2\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 145, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[150000,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "fim = 70\n",
    "for metodo in metodos:\n",
    "\n",
    "    lista_df_resultado = []\n",
    "    \n",
    "    for nome, dataset in zip(arquivos, lista_train_test_valid):    \n",
    "        \n",
    "        X_train, y_train, X_test, y_test, X_valid, y_valid = dataset\n",
    "\n",
    "        start_time = time.time()\n",
    "        #(nome, historico, y_test, y_pred, modelo) = fmod.pipeline_bert(metodo, nome, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "        (nome, historico, y_test, y_pred, modelo) = fmod.pipeline_bert(metodo, nome, X_train[:fim], y_train[:fim], X_valid[:fim], y_valid[:fim], X_test[:fim], y_test[:fim])\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        \n",
    "        pd.DataFrame.from_dict(historico.history).to_csv(f'Dados/Resultados/{metodo}/{nome}_historico.csv', index = False)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict = True)\n",
    "        \n",
    "        df_resultado = salvar_df_resultado(report, metodo, nome, runtime)\n",
    "        salvar_modelo_bert(modelo, nome, metodo)\n",
    "\n",
    "        lista_df_resultado.append(df_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW Co ocorrencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''metodo = \"BoW\"\n",
    "lista_df_resultado = []\n",
    "fim = 150\n",
    "\n",
    "for nome, dataset in zip(arquivos, lista_train_test_valid):\n",
    "    \n",
    "    #print(f\"\\n\\nNome: {nome}\\n\\n\")\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    name_dataset, y_test, y_pred, modelo = fbow.pipeline_rf(nome, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "    # name_dataset, y_test, y_pred = fbow.pipeline_rf(nome, X_train[:fim], y_train[:fim], X_valid[:fim], y_valid[:fim], X_test[:fim], y_test[:fim])\n",
    "    \n",
    "    #print(f\"\\n\\n{nome}\\n\\n\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "    \n",
    "    df_resultado = salvar_df_resultado(report, metodo, nome, runtime)\n",
    "    salvar_modelo(modelo, nome, \"BoWCo\")\n",
    "\n",
    "    lista_df_resultado.append(df_resultado)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''metodo = \"Cos\"\n",
    "lista_df_resultado = []\n",
    "thresh_list = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for nome, dataset in zip(arquivos, lista_train_test_valid):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset\n",
    "\n",
    "    for threshold in thresh_list:\n",
    "        start_time = time.time()\n",
    "\n",
    "        name_dataset, y_test, y_pred = fcos.cos_threshold(nome, X_test, y_test, threshold)\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict = True)\n",
    "        \n",
    "        df_resultado = salvar_df_resultado(report, f'{metodo}_0{int(threshold*10)}', nome, runtime)\n",
    "\n",
    "        lista_df_resultado.append(df_resultado)'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(lista_df_resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4889b298c2b41a0f37c917b2884deb5a3a36b040cfb99d0e8c1edb9e5e6fadf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
