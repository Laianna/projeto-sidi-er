{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import funcoes_modelos as fmod\n",
    "import funcoes_bow as fbow\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "flag_cpu = True\n",
    "if flag_cpu == True:\n",
    "    # force CPU (make CPU visible)\n",
    "    cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "    print(cpus)\n",
    "    tf.config.set_visible_devices([], 'GPU')  # hide the GPU\n",
    "    tf.config.set_visible_devices(cpus[0], 'CPU') # unhide potentially hidden CPU\n",
    "    tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = ['hn_balanceado', 'hn_desbalanceado', 'sn_balanceado', 'sn_desbalanceado']\n",
    "\n",
    "lista_df = []\n",
    "for arquivo in arquivos:\n",
    "\n",
    "    df = pd.read_csv(f\"Dados/Datasets/{arquivo}.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    lista_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_df[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho dos Datasets:\n",
      "\n",
      "\t\t| Hard\t| Soft\t|\n",
      "Balanceado\t| 8400\t| 8400\t|\n",
      "Desbalanceado\t| 13290\t| 13290\t|\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamanho dos Datasets:\\n\\n\\t\\t| Hard\\t| Soft\\t|\\nBalanceado\\t| {lista_df[0].shape[0]}\\t| {lista_df[2].shape[0]}\\t|\\nDesbalanceado\\t| {lista_df[1].shape[0]}\\t| {lista_df[3].shape[0]}\\t|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_valid(df):\n",
    "    \n",
    "    X = df[[\"titulo_1\", \"titulo_2\"]]\n",
    "    y = df[\"match\"].to_list()\n",
    "    \n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED, stratify = y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.285, random_state = SEED, stratify = y_train_valid)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_train_test_valid = []\n",
    "for df in lista_df:\n",
    "    lista_train_test_valid.append(train_test_valid(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome, dataset in zip(arquivos, lista_train_test_valid):    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos Com Os Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df_resultado(report, modelo, nome_dataset, tempo_exec):\n",
    "    \n",
    "    df_resultado = pd.DataFrame(report).transpose()\n",
    "    df_resultado['modelo'] = modelo\n",
    "    df_resultado['dataset'] = nome_dataset\n",
    "    df_resultado['tempo'] = tempo_exec\n",
    "\n",
    "    df_resultado.to_csv(f'Dados/Resultados/{modelo}/{nome_dataset}_resultado.csv', index = True)\n",
    "    \n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT, roBERTa, XLMR e ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2167009006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = (\"BERT\", \"roBERTa\", \"XLMR\", \"ELECTRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelo in modelos:\n",
    "    \n",
    "    lista_df_resultado = []\n",
    "    \n",
    "    for nome, dataset in zip(arquivos, lista_train_test_valid):    \n",
    "        \n",
    "        X_train, y_train, X_test, y_test, X_valid, y_valid = dataset\n",
    "\n",
    "        start_time = time.time()\n",
    "        (nome, historico, y_test, y_pred) = fmod.pipeline_bert(modelo, nome, X_train[:5], y_train[:5], X_valid[:5], y_valid[:5], X_test[:5], y_test[:5])\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        \n",
    "        pd.DataFrame.from_dict(historico.history).to_csv(f'Dados/Resultados/{modelo}/{nome}_historico.csv', index = False)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict = True)\n",
    "        \n",
    "        df_resultado = salvar_df_resultado(report, modelo, nome, runtime)\n",
    "\n",
    "        lista_df_resultado.append(df_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pontuacao(texto):\n",
    "    \n",
    "    #texto_sp = \"\".join([i for i in texto if i not in string.punctuation])\n",
    "    texto_sp = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return texto_sp\n",
    "\n",
    "\n",
    "#Tranforma o dataframe de dados em uma lista única de sentenças\n",
    "def get_list_dataframe(dataframe):\n",
    "    \n",
    "    lista_titulos = []\n",
    "    for titulo_1, titulo_2 in zip(dataframe[\"titulo_1\"], dataframe[\"titulo_2\"]):\n",
    "\n",
    "        lista_titulos.append(remove_pontuacao(titulo_1))\n",
    "        lista_titulos.append(remove_pontuacao(titulo_2))\n",
    "        \n",
    "    return lista_titulos\n",
    "\n",
    "\n",
    "#Cria bow binário das sentenças\n",
    "def vectorize_dataframe(dataframe):\n",
    "    lista_titulos = get_list_dataframe(dataframe)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                lowercase = True,\n",
    "                                strip_accents = 'unicode',\n",
    "                                binary = True\n",
    "                                ) \n",
    "\n",
    "    vector = vectorizer.fit_transform(lista_titulos).toarray()\n",
    "\n",
    "    return vector, vectorizer\n",
    "\n",
    "\n",
    "#Cria a matrix de coocorrencia do título1 e título2 \n",
    "def get_cooccurrence_bow(dataframe):\n",
    "    \n",
    "    vector, vectorizer = vectorize_dataframe(dataframe)\n",
    "    lista_features = []\n",
    "\n",
    "    for indice in range(0, len(vector), 2):\n",
    "        \n",
    "        vec_titulo_1 = vector[indice]\n",
    "        vec_titulo_2 = vector[indice + 1]\n",
    "        \n",
    "        lista_coo = np.multiply(np.logical_and(vec_titulo_1, vec_titulo_2), 1).tolist()\n",
    "        \n",
    "        lista_features.append(lista_coo)\n",
    "\n",
    "    return lista_features, vector, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fim = 50\n",
    "\n",
    "modelo = \"BoW\"\n",
    "\n",
    "for nome, dataset in zip(arquivos, lista_train_test_valid):    \n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset\n",
    "\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    name_dataset, y_test, label_pred = fbow.pipeline_rf(nome, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "    #name_dataset, y_test, label_pred = fbow.pipeline_rf(nome, X_train[:fim], y_train[:fim], X_valid[:fim], y_valid[:fim], X_test[:fim], y_test[:fim])\n",
    "    \n",
    "    #print(f\"\\n\\n{nome}\\n\\n\")\n",
    "\n",
    "    break\n",
    "    '''end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "    \n",
    "    df_resultado = salvar_df_resultado(report, modelo, nome, runtime)\n",
    "\n",
    "    lista_df_resultado.append(df_resultado)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      2100\n",
      "           1       0.00      0.00      0.00       420\n",
      "\n",
      "    accuracy                           0.83      2520\n",
      "   macro avg       0.42      0.50      0.45      2520\n",
      "weighted avg       0.69      0.83      0.76      2520\n",
      "\n",
      "[[2100    0]\n",
      " [ 420    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lucas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lucas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, label_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurrence_bow_test_val(cv_treino, vectorizer_treino, X_):\n",
    "    \n",
    "    co_, vec_, cv_ = get_cooccurrence_bow(X_)\n",
    "    \n",
    "    co_vector = np.zeros((len(co_), len(vectorizer_treino.vocabulary_)), dtype = np.int64).tolist()\n",
    "    \n",
    "    for linha in range(len(co_)):\n",
    "\n",
    "        for chave, dic_indice in zip(cv_.vocabulary_.keys(), cv_.vocabulary_.values()):\n",
    "            \n",
    "            if chave in cv_treino.vocabulary_:\n",
    "                \n",
    "                co_vector[linha][dic_indice] = co_[linha][dic_indice]\n",
    "\n",
    "\n",
    "                             \n",
    "    return co_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(co_teste[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (chave, dic_indice) in enumerate(zip(cv_treino.vocabulary_.keys(), cv_treino.vocabulary_.values())):\n",
    "    \n",
    "    print(i)\n",
    "    print(chave)\n",
    "    print(dic_indice)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_['smartphone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lista_df_resultado = []\n",
    "for nome, df in zip(arquivos, lista_df):\n",
    "    \n",
    "    X = df[[\"titulo_1\", \"titulo_2\"]]#[df[\"titulo_1\"].to_list(), df[\"titulo_2\"].to_list()]\n",
    "    y = df[\"match\"].to_list()\n",
    "\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED, stratify = y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2, random_state = SEED, stratify = y_train_valid)\n",
    "\n",
    "    (nome, historico, y_test, y_pred) = fmod.pipeline_bert(nome, X_train[:5], y_train[:5], X_valid[:5], y_valid[:5], X_test[:5], y_test[:5])\n",
    "\n",
    "    pd.DataFrame.from_dict(historico.history).to_csv(f'Dados/Resultados/BERTo/{nome}_historico.csv', index = False)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "    df_resultado = pd.DataFrame(report).transpose()\n",
    "    df_resultado['modelo'] = nome\n",
    "\n",
    "    break\n",
    "\n",
    "    #df_resultado.to_csv(f'Dados/Resultados/BERTo/{nome}_resultado.csv', index = True)\n",
    "\n",
    "    lista_df_resultado.append(df_resultado)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
